{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalsalilan/B3-Developing-Named-Entity-Recognition-NER-Models-for-Financial-Data-Extraction-/blob/Naveen/LangExtract_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpmsiaT5Z3sm",
        "outputId": "2f48827f-1feb-4ed2-c10a-4fbd28ee6a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.60.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.11.10)\n",
            "Collecting docling-core<3.0.0,>=2.48.2 (from docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading docling_core-2.50.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.11.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2025.10.5)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (5.4.0)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.11.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-2.22.3-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.8.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (4.25.1)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.48.2->docling) (4.57.1)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.23.0+cu126)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-37.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.48.2->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.28.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.22.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.48.2->docling) (0.3.8)\n",
            "Downloading docling-2.60.1-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.50.1-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-2.22.3-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.4.2-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.12.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=df6533f1e0eee4d1472a71239e8d9a88f05034aedeb7fbef54b942ef7b852f3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, pyclipper, filetype, XlsxWriter, rtree, python-docx, pypdfium2, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.20.0\n",
            "    Uninstalling typer-0.20.0:\n",
            "      Successfully uninstalled typer-0.20.0\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.60.1 docling-core-2.50.1 docling-ibm-models-3.10.2 docling-parse-4.7.0 faker-37.12.0 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.1 mpire-2.10.2 polyfactory-2.22.3 pyclipper-1.3.0.post6 pylatexenc-2.10 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.4.2 rtree-1.4.1 semchunk-2.2.2 typer-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiFVfO2bbM6k",
        "outputId": "a010b556-f605-4180-9808-4af2aa1e5fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langextract\n",
            "  Downloading langextract-1.0.9-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (3.13.1)\n",
            "Collecting async_timeout>=4.0.0 (from langextract)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting exceptiongroup>=1.1.0 (from langextract)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-genai>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.46.0)\n",
            "Collecting ml-collections>=0.1.0 (from langextract)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: more-itertools>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (10.8.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.11.10)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.2.1)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.22.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2025.10.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai>=0.1.0->langextract) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->langextract) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.6.1)\n",
            "Downloading langextract-1.0.9-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-collections, exceptiongroup, async_timeout, langextract\n",
            "Successfully installed async_timeout-5.0.1 exceptiongroup-1.3.0 langextract-1.0.9 ml-collections-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "hXjWW6hIgP3n",
        "outputId": "62186988-b990-4c93-aa16-632cfdd36336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully retrieved API key from Colab secrets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mextraction_results.jsonl\u001b[0m: 1 docs [00:00, 1275.64 docs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mextraction_results.jsonl\u001b[0m: 100%|██████████| 716/716 [00:00<00:00, 1.90MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_375d77c9-ca51-4bb7-9f04-b27b724bc216\", \"visualization.html\", 7439)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed. Check 'extraction_results.jsonl' and 'visualization.html'.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install required libraries\n",
        "# !pip install langextract     # latest on PyPI version 1.0.9 :contentReference[oaicite:2]{index=2}\n",
        "# !pip install docling         # if you’ll use Docling to parse PDFs/Word to text :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "# 2. Imports\n",
        "import os\n",
        "import textwrap\n",
        "import langextract as lx\n",
        "# optional: for doc parsing\n",
        "# from docling.document_converter import DocumentConverter # Docling import issues were noted previously\n",
        "\n",
        "# 3. (Optional) If you need to extract text from a document\n",
        "# Disabling Docling part due to import errors in previous attempts\n",
        "# def load_document_text(path_or_url: str) -> str:\n",
        "#     \"\"\"Use Docling to load a doc (PDF/DOCX/HTML) into a plain-text string.\"\"\"\n",
        "#     converter = DocumentConverter()\n",
        "#     doc = converter.convert(path_or_url)\n",
        "#     return doc.document.export_to_markdown()  # or .export_to_text() depending on version\n",
        "\n",
        "# Example usage:\n",
        "# input_text = load_document_text(\"path/to/contract.pdf\")\n",
        "\n",
        "# 4. Define prompt and few-shot examples for your extraction task\n",
        "# (Adapt to your domain: e.g., “termination clause”, “governing law”, “parties”, etc.)\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Extract the termination clause from the document using the **exact text**.\n",
        "    Provide for each extraction:\n",
        "      - extraction_class: \"termination_clause\"\n",
        "      - extraction_text: the exact clause text\n",
        "      - attributes: { \"type\": ..., \"triggering_party\": ..., \"notice_period\": ..., \"effect\": ... }\n",
        "    Use the exact text, do not paraphrase or omit any part of the clause.\n",
        "\"\"\")\n",
        "\n",
        "examples = [\n",
        "    lx.data.ExampleData(\n",
        "        text=\"If either party defaults on its obligations, the other party may terminate this Agreement by giving thirty (30) days’ written notice.\",\n",
        "        extractions=[\n",
        "            lx.data.Extraction(\n",
        "                extraction_class=\"termination_clause\",\n",
        "                extraction_text=\"If either party defaults on its obligations, the other party may terminate this Agreement by giving thirty (30) days’ written notice.\",\n",
        "                attributes={\n",
        "                    \"type\": \"Termination for Cause\",\n",
        "                    \"triggering_party\": \"Either Party\",\n",
        "                    \"notice_period\": \"30 days\",\n",
        "                    \"condition\": \"default on obligations\",\n",
        "                    \"effect\": \"Agreement termination\"\n",
        "                }\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    lx.data.ExampleData(\n",
        "        text=\"The Client may terminate this Contract at any time for convenience by providing forty-five (45) days’ written notice to the Service Provider.\",\n",
        "        extractions=[\n",
        "            lx.data.Extraction(\n",
        "                extraction_class=\"termination_clause\",\n",
        "                extraction_text=\"The Client may terminate this Contract at any time for convenience by providing forty-five (45) days’ written notice to the Service Provider.\",\n",
        "                attributes={\n",
        "                    \"type\": \"Termination for Convenience\",\n",
        "                    \"triggering_party\": \"Client\",\n",
        "                    \"notice_period\": \"45 days\",\n",
        "                    \"condition\": \"for convenience\",\n",
        "                    \"effect\": \"Contract termination\"\n",
        "                }\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 5. Load your document text\n",
        "input_text = \"\"\"This agreement is effective from today. The term is one year. Either party may terminate this agreement with 30 days written notice. Upon termination, all outstanding fees are due.\"\"\"\n",
        "# or from file:\n",
        "# input_text = open(\"contract.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# 6. Set API key (if using cloud LLM)\n",
        "# Either set environment variable in Colab:\n",
        "# os.environ[\"LANGEXTRACT_API_KEY\"] = GOOGLE_API_KEY # This is likely the source of the error as GOOGLE_API_KEY is not defined here\n",
        "\n",
        "# Correctly access the API key from Colab secrets\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if not google_api_key:\n",
        "        raise ValueError(\"GOOGLE_API_KEY not found in Colab secrets.\")\n",
        "    os.environ[\"LANGEXTRACT_API_KEY\"] = google_api_key\n",
        "    print(\"Successfully retrieved API key from Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Colab secrets or API key: {e}\")\n",
        "    print(\"Please ensure you have added your API key to Colab secrets with the name 'GOOGLE_API_KEY'.\")\n",
        "    # Exit or handle the error appropriately if the API key is essential\n",
        "\n",
        "# 7. Run the extraction\n",
        "if 'google_api_key' in locals() and google_api_key: # Only run extraction if API key was successfully retrieved\n",
        "    result = lx.extract(\n",
        "        text_or_documents=input_text,\n",
        "        prompt_description=prompt,\n",
        "        examples=examples,\n",
        "        model_id=\"gemini-2.5-flash\",      # recommended default :contentReference[oaicite:4]{index=4}\n",
        "        extraction_passes=2,              # optional: helps recall for longer docs\n",
        "        max_workers=4                     # optional: parallelism if many docs\n",
        "    )\n",
        "\n",
        "    # 8. Save results to JSONL\n",
        "    lx.io.save_annotated_documents(\n",
        "        [result],\n",
        "        output_name=\"extraction_results.jsonl\",\n",
        "        output_dir=\".\"\n",
        "    )\n",
        "\n",
        "    # 9. Generate interactive visualization (HTML)\n",
        "    html_content = lx.visualize(\"extraction_results.jsonl\")\n",
        "    with open(\"visualization.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        if hasattr(html_content, \"data\"):\n",
        "            f.write(html_content.data)\n",
        "        else:\n",
        "            f.write(html_content)\n",
        "\n",
        "    # 10. (Optional) Download visualization in Colab\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        files.download(\"visualization.html\")\n",
        "        print(\"Extraction completed. Check 'extraction_results.jsonl' and 'visualization.html'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading visualization file: {e}\")\n",
        "        print(\"Extraction completed. Check 'extraction_results.jsonl' and 'visualization.html' in the files pane.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping langextract execution due to missing or invalid API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzDVxkd9xEK-"
      },
      "source": [
        "Below is app for the same (LangExtract + Docling + Docling) using gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8DwLHXCyunbH",
        "outputId": "ac8a0511-93f4-4b86-d29c-e9b393c34b27"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7cb9c60dcbd9650c1f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7cb9c60dcbd9650c1f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mextraction_results\u001b[0m: 1 docs [00:00, 435.95 docs/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mextraction_results\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mextraction_results.jsonl\u001b[0m: 100%|██████████| 716/716 [00:00<00:00, 1.85MB/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2957\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1149406546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mclear_btn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstatus_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_preview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml_preview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml_download\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2863\u001b[0m             )\n\u001b[1;32m   2864\u001b[0m         ):\n\u001b[0;32m-> 2865\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTupleNoPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mblock_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2960\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyboard interruption in main thread... closing server.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2962\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtunnel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCURRENT_TUNNELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m                 \u001b[0mtunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# COMPLETE, FIXED COLAB NOTEBOOK\n",
        "# LangExtract + Docling + Gradio app\n",
        "# (fixes AnnotatedDocument serialization issues)\n",
        "# ==========================================\n",
        "\n",
        "# STEP 1: install dependencies\n",
        "!pip install -q langextract docling gradio\n",
        "\n",
        "# STEP 2: imports\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import langextract as lx\n",
        "import gradio as gr\n",
        "from docling.document_converter import DocumentConverter\n",
        "from typing import Tuple\n",
        "\n",
        "# STEP 3: set your Gemini API key (LangExtract will use it)\n",
        "# Replace the string below with your Gemini API key (or set in environment separately)\n",
        "os.environ[\"LANGEXTRACT_API_KEY\"] = \"AIzaSyAHszWqcmRrgq0ejAh4DX0FzfYN7u_KBwI\"\n",
        "\n",
        "\n",
        "# STEP 4: robust document parsing using Docling\n",
        "def parse_document(uploaded_file) -> str:\n",
        "    \"\"\"\n",
        "    Parse DOCX / PDF / TXT uploads to plain text using Docling.\n",
        "    Handles Gradio's different upload object shapes (NamedString, tempfile, file-like).\n",
        "    Returns the extracted text or raises an Exception string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        converter = DocumentConverter()\n",
        "\n",
        "        # Accept file-like (has read), NamedString (has .data), or object with .name path\n",
        "        if hasattr(uploaded_file, \"read\"):\n",
        "            file_bytes = uploaded_file.read()\n",
        "        elif hasattr(uploaded_file, \"data\"):\n",
        "            file_bytes = uploaded_file.data\n",
        "        elif hasattr(uploaded_file, \"name\"):\n",
        "            with open(uploaded_file.name, \"rb\") as f:\n",
        "                file_bytes = f.read()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type from uploader\")\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
        "            tmp.write(file_bytes)\n",
        "            tmp.flush()\n",
        "            # Let Docling convert the temporary file\n",
        "            doc = converter.convert(tmp.name)\n",
        "\n",
        "        # doc.document.export_to_text() is robust; you can also try .export_to_markdown()\n",
        "        text = doc.document.export_to_text()\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return an error string so UI can show it\n",
        "        return f\"Error parsing document: {e}\"\n",
        "\n",
        "\n",
        "# STEP 5: few-shot examples to help LangExtract\n",
        "def get_few_shot_examples():\n",
        "    examples = [\n",
        "        lx.data.ExampleData(\n",
        "            text=(\n",
        "                \"If either party defaults on its obligations under this Agreement, \"\n",
        "                \"the other party may terminate this Agreement by giving thirty (30) \"\n",
        "                \"days’ written notice.\"\n",
        "            ),\n",
        "            extractions=[\n",
        "                lx.data.Extraction(\n",
        "                    extraction_class=\"termination_clause\",\n",
        "                    extraction_text=(\n",
        "                        \"If either party defaults on its obligations under this Agreement, \"\n",
        "                        \"the other party may terminate this Agreement by giving thirty (30) \"\n",
        "                        \"days’ written notice.\"\n",
        "                    ),\n",
        "                    attributes={\n",
        "                        \"type\": \"Termination for Cause\",\n",
        "                        \"triggering_party\": \"Either Party\",\n",
        "                        \"notice_period\": \"30 days\",\n",
        "                        \"condition\": \"default on obligations\",\n",
        "                        \"effect\": \"Agreement termination\"\n",
        "                    }\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "        lx.data.ExampleData(\n",
        "            text=\"The Client may terminate this Contract at any time for convenience by providing forty-five (45) days’ written notice to the Service Provider.\",\n",
        "            extractions=[\n",
        "                lx.data.Extraction(\n",
        "                    extraction_class=\"termination_clause\",\n",
        "                    extraction_text=\"The Client may terminate this Contract at any time for convenience by providing forty-five (45) days’ written notice to the Service Provider.\",\n",
        "                    attributes={\n",
        "                        \"type\": \"Termination for Convenience\",\n",
        "                        \"triggering_party\": \"Client\",\n",
        "                        \"notice_period\": \"45 days\",\n",
        "                        \"condition\": \"for convenience\",\n",
        "                        \"effect\": \"Contract termination\"\n",
        "                    }\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "    ]\n",
        "    return examples\n",
        "\n",
        "\n",
        "# STEP 6: run LangExtract and produce previewable files\n",
        "def run_extraction(document_text: str, prompt: str) -> Tuple[str, str, str, str, str]:\n",
        "    \"\"\"\n",
        "    Perform extraction using LangExtract (Gemini backend via LANGEXTRACT_API_KEY).\n",
        "    Returns tuple:\n",
        "      (status_message, preview_json_string, html_string, jsonl_filename, html_filename)\n",
        "    In case of failure returns (error_message, None, None, None, None)\n",
        "    \"\"\"\n",
        "    if not os.getenv(\"LANGEXTRACT_API_KEY\"):\n",
        "        return \"❌ Missing Gemini API key. Please set LANGEXTRACT_API_KEY.\", None, None, None, None\n",
        "\n",
        "    try:\n",
        "        examples = get_few_shot_examples()\n",
        "\n",
        "        # Run extraction. May return single AnnotatedDocument or a list.\n",
        "        result = lx.extract(\n",
        "            text_or_documents=document_text,\n",
        "            prompt_description=prompt,\n",
        "            examples=examples,\n",
        "            model_id=\"gemini-2.5-flash\",\n",
        "            # model_id=\"gemini-1.5-flash\",\n",
        "            extraction_passes=3,\n",
        "            max_workers=4\n",
        "        )\n",
        "\n",
        "        # Normalize to list\n",
        "        annotated_docs = result if isinstance(result, list) else [result]\n",
        "\n",
        "        # Save annotated documents to JSONL using the stable IO helper\n",
        "        # Note: output_name should be a base name, the library appends .jsonl\n",
        "        out_basename = \"extraction_results\"\n",
        "        lx.io.save_annotated_documents(annotated_docs, output_name=out_basename, output_dir=\".\")\n",
        "        jsonl_path = f\"{out_basename}.jsonl\"\n",
        "\n",
        "        # Read JSONL for preview (safe across versions)\n",
        "        preview_list = []\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    preview_list.append(json.loads(line))\n",
        "                except Exception:\n",
        "                    # If line is not strict JSON, keep raw\n",
        "                    preview_list.append({\"raw_line\": line})\n",
        "\n",
        "        preview_json = json.dumps(preview_list, indent=2)\n",
        "\n",
        "        # Generate visualization HTML\n",
        "        html_obj = lx.visualize(jsonl_path)\n",
        "        html_content = html_obj.data if hasattr(html_obj, \"data\") else str(html_obj)\n",
        "        html_path = \"visualization.html\"\n",
        "        with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        return (\n",
        "            \"✅ Extraction complete! Use the previews below or download the files.\",\n",
        "            preview_json,\n",
        "            html_content,\n",
        "            jsonl_path,\n",
        "            html_path\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        # Surface the real error for debugging\n",
        "        return f\"❌ Extraction failed: {e}\", None, None, None, None\n",
        "\n",
        "\n",
        "# STEP 7: Gradio interface wrapper\n",
        "def app_interface(uploaded_file, user_prompt):\n",
        "    if uploaded_file is None:\n",
        "        return \"⚠️ Please upload a document.\", None, None, None, None\n",
        "    if not user_prompt or not user_prompt.strip():\n",
        "        return \"⚠️ Please enter a prompt.\", None, None, None, None\n",
        "\n",
        "    # Parse\n",
        "    parsed = parse_document(uploaded_file)\n",
        "    if isinstance(parsed, str) and parsed.startswith(\"Error parsing document\"):\n",
        "        return parsed, None, None, None, None\n",
        "\n",
        "    # Run extraction\n",
        "    return run_extraction(parsed, user_prompt)\n",
        "\n",
        "\n",
        "# STEP 8: build the Gradio UI (previews + downloads)\n",
        "with gr.Blocks(title=\"LangExtract + Docling Dynamic Extraction (Fixed Serialization)\") as demo:\n",
        "    gr.Markdown(\"## LangExtract + Docling Dynamic Extraction App (serialization-safe)\")\n",
        "    gr.Markdown(\"Upload a document, provide a prompt, preview JSON and HTML results, or download output files.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"📄 Upload document (PDF/DOCX/TXT)\")\n",
        "        prompt_input = gr.Textbox(\n",
        "            label=\"🧾 Extraction prompt\",\n",
        "            placeholder=\"e.g. Extract parties, effective date, termination, payment terms, governing law...\",\n",
        "            lines=6\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Run Extraction\")\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        json_preview = gr.Code(label=\"JSONL Preview\", language=\"json\")\n",
        "        html_preview = gr.HTML(label=\"HTML Visualization Preview\")\n",
        "\n",
        "    with gr.Row():\n",
        "        json_download = gr.File(label=\"Download JSONL\")\n",
        "        html_download = gr.File(label=\"Download HTML visualization\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=app_interface,\n",
        "        inputs=[file_input, prompt_input],\n",
        "        outputs=[status_box, json_preview, html_preview, json_download, html_download],\n",
        "    )\n",
        "\n",
        "    clear_btn.click(lambda: (\"\", \"\", \"\", None, None), None, [status_box, json_preview, html_preview, json_download, html_download])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# LangExtract + Docling + Gradio App (Fixed)\n",
        "# ==========================================\n",
        "\n",
        "# STEP 1: install dependencies\n",
        "# !pip install -q langextract docling gradio\n",
        "\n",
        "# STEP 2: imports\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import textwrap\n",
        "import langextract as lx\n",
        "import gradio as gr\n",
        "from docling.document_converter import DocumentConverter\n",
        "from typing import Tuple, List\n",
        "\n",
        "# STEP 3: set your Gemini API key\n",
        "os.environ[\"LANGEXTRACT_API_KEY\"] = \"AIzaSyAHszWqcmRrgq0ejAh4DX0FzfYN7u_KBwI\"  # <-- replace this\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: Improved document parsing\n",
        "# ==========================================\n",
        "def parse_document(uploaded_file) -> str:\n",
        "    \"\"\"\n",
        "    Parse DOCX / PDF / TXT uploads to plain text using Docling.\n",
        "    Ensures full text is extracted even from multi-page or formatted documents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        converter = DocumentConverter()\n",
        "        file_bytes = None\n",
        "\n",
        "        if hasattr(uploaded_file, \"read\"):\n",
        "            file_bytes = uploaded_file.read()\n",
        "        elif hasattr(uploaded_file, \"data\"):\n",
        "            file_bytes = uploaded_file.data\n",
        "        elif hasattr(uploaded_file, \"name\"):\n",
        "            with open(uploaded_file.name, \"rb\") as f:\n",
        "                file_bytes = f.read()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type from uploader\")\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tmp\") as tmp:\n",
        "            tmp.write(file_bytes)\n",
        "            tmp.flush()\n",
        "            doc = converter.convert(tmp.name)\n",
        "\n",
        "        # Try plain text first, fallback to markdown if too short\n",
        "        text = doc.document.export_to_text()\n",
        "        if len(text.strip().split()) < 20 and hasattr(doc.document, \"export_to_markdown\"):\n",
        "            text = doc.document.export_to_markdown()\n",
        "\n",
        "        # Clean up extra whitespace\n",
        "        text = \" \".join(text.split())\n",
        "        if not text or len(text) < 50:\n",
        "            raise ValueError(\"Document text appears empty or incomplete.\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing document: {e}\"\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: Few-shot examples for LangExtract\n",
        "# ==========================================\n",
        "def get_few_shot_examples():\n",
        "    return [\n",
        "        lx.data.ExampleData(\n",
        "            text=\"Either party may terminate this Agreement with thirty (30) days’ written notice.\",\n",
        "            extractions=[\n",
        "                lx.data.Extraction(\n",
        "                    extraction_class=\"termination_clause\",\n",
        "                    extraction_text=\"Either party may terminate this Agreement with thirty (30) days’ written notice.\",\n",
        "                    attributes={\n",
        "                        \"type\": \"Termination for Convenience\",\n",
        "                        \"triggering_party\": \"Either Party\",\n",
        "                        \"notice_period\": \"30 days\",\n",
        "                        \"condition\": \"for convenience\",\n",
        "                        \"effect\": \"Agreement termination\"\n",
        "                    }\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "        lx.data.ExampleData(\n",
        "            text=\"If either party defaults on its obligations, the other may terminate this Agreement immediately.\",\n",
        "            extractions=[\n",
        "                lx.data.Extraction(\n",
        "                    extraction_class=\"termination_clause\",\n",
        "                    extraction_text=\"If either party defaults on its obligations, the other may terminate this Agreement immediately.\",\n",
        "                    attributes={\n",
        "                        \"type\": \"Termination for Cause\",\n",
        "                        \"triggering_party\": \"Either Party\",\n",
        "                        \"notice_period\": \"immediate\",\n",
        "                        \"condition\": \"default on obligations\",\n",
        "                        \"effect\": \"Contract termination\"\n",
        "                    }\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: Chunk large text safely\n",
        "# ==========================================\n",
        "def chunk_text(text: str, chunk_size: int = 8000, overlap: int = 250) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits a long document text into smaller chunks with overlap.\n",
        "    Helps avoid token limit and improves coverage for multi-page docs.\n",
        "    \"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: Run LangExtract\n",
        "# ==========================================\n",
        "def run_extraction(document_text: str, prompt: str) -> Tuple[str, str, str, str, str]:\n",
        "    \"\"\"\n",
        "    Runs extraction using LangExtract (Gemini backend).\n",
        "    Handles chunked processing for large texts.\n",
        "    \"\"\"\n",
        "    if not os.getenv(\"LANGEXTRACT_API_KEY\"):\n",
        "        return \"❌ Missing Gemini API key. Please set LANGEXTRACT_API_KEY.\", None, None, None, None\n",
        "\n",
        "    try:\n",
        "        examples = get_few_shot_examples()\n",
        "        chunks = chunk_text(document_text)\n",
        "        all_docs = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            result = lx.extract(\n",
        "                text_or_documents=chunk,\n",
        "                prompt_description=prompt,\n",
        "                examples=examples,\n",
        "                model_id=\"gemini-2.5-flash\",  # use lighter model to avoid quota errors\n",
        "                extraction_passes=1,\n",
        "                max_workers=1\n",
        "            )\n",
        "            if isinstance(result, list):\n",
        "                all_docs.extend(result)\n",
        "            else:\n",
        "                all_docs.append(result)\n",
        "\n",
        "        # Save annotated documents\n",
        "        out_basename = \"extraction_results\"\n",
        "        lx.io.save_annotated_documents(all_docs, output_name=out_basename, output_dir=\".\")\n",
        "        jsonl_path = f\"{out_basename}.jsonl\"\n",
        "\n",
        "        # Load JSON for preview\n",
        "        preview_list = []\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    preview_list.append(json.loads(line.strip()))\n",
        "                except:\n",
        "                    continue\n",
        "        preview_json = json.dumps(preview_list, indent=2)\n",
        "\n",
        "        # Generate visualization\n",
        "        html_obj = lx.visualize(jsonl_path)\n",
        "        html_content = html_obj.data if hasattr(html_obj, \"data\") else str(html_obj)\n",
        "        html_path = \"visualization.html\"\n",
        "        with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        return (\n",
        "            f\"✅ Extraction complete! Processed {len(chunks)} chunk(s).\",\n",
        "            preview_json,\n",
        "            html_content,\n",
        "            jsonl_path,\n",
        "            html_path,\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Extraction failed: {e}\", None, None, None, None\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8: Gradio Interface\n",
        "# ==========================================\n",
        "def app_interface(uploaded_file, user_prompt):\n",
        "    if uploaded_file is None:\n",
        "        return \"⚠️ Please upload a document.\", None, None, None, None\n",
        "    if not user_prompt.strip():\n",
        "        return \"⚠️ Please enter a prompt.\", None, None, None, None\n",
        "\n",
        "    parsed = parse_document(uploaded_file)\n",
        "    if isinstance(parsed, str) and parsed.startswith(\"Error parsing document\"):\n",
        "        return parsed, None, None, None, None\n",
        "\n",
        "    return run_extraction(parsed, user_prompt)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9: Launch Gradio UI\n",
        "# ==========================================\n",
        "with gr.Blocks(title=\"LangExtract + Docling Dynamic Extraction (Full Document)\") as demo:\n",
        "    gr.Markdown(\"## 📄 LangExtract + Docling Extraction App (Full Document Fix)\")\n",
        "    gr.Markdown(\"Upload a legal or contract document, enter your extraction prompt, and get highlighted JSON + HTML output.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"📄 Upload document (PDF/DOCX/TXT)\")\n",
        "        prompt_input = gr.Textbox(\n",
        "            label=\"🧾 Extraction prompt\",\n",
        "            placeholder=\"Example: Extract all termination clauses and related details...\",\n",
        "            lines=5\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"▶️ Run Extraction\")\n",
        "        clear_btn = gr.Button(\"🧹 Clear\")\n",
        "\n",
        "    status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
        "    json_preview = gr.Code(label=\"📊 JSONL Preview\", language=\"json\")\n",
        "    html_preview = gr.HTML(label=\"💡 Visualization Preview\")\n",
        "\n",
        "    with gr.Row():\n",
        "        json_download = gr.File(label=\"⬇️ Download JSONL\")\n",
        "        html_download = gr.File(label=\"⬇️ Download HTML\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=app_interface,\n",
        "        inputs=[file_input, prompt_input],\n",
        "        outputs=[status_box, json_preview, html_preview, json_download, html_download],\n",
        "    )\n",
        "\n",
        "    clear_btn.click(lambda: (\"\", \"\", \"\", None, None), None, [status_box, json_preview, html_preview, json_download, html_download])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "7zyDZdfBC-yD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEHyjMvqJAghun7k55uJDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}